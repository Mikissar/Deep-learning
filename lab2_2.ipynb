{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQyEXRilonAP"
   },
   "source": [
    "https://udlbook.github.io/udlbook/\n",
    "\n",
    "https://github.com/udlbook/udlbook/blob/main/Notebooks/Chap07/7_2_Backpropagation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6chybAVFJW2"
   },
   "source": [
    "# **Блокнот 7.2: Обратное распространение (backpropagation)**\n",
    "\n",
    "В этом блокноте выполняется алгоритм обратного распространения ошибки в глубокой нейронной сети, как описано в разделе 7.4 книги.\n",
    "\n",
    "Пройдитесь по ячейкам ниже, запуская каждую ячейку по очереди. В разных местах вы увидите метку \"TODO\". Следуйте инструкциям в этих местах и сделайте прогнозы о том, что должно произойти, или напишите код для выполнения функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LdIDglk1FFcG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnUoI0m6GyjC"
   },
   "source": [
    "Сначала давайте определим нейронную сеть. Пока мы просто выберем веса и смещения случайным образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WVM4Tc_jGI0Q"
   },
   "outputs": [],
   "source": [
    "# Установим начальное значение (seed), чтобы мы всегда получали одни и те же случайные числа\n",
    "np.random.seed(0)\n",
    "\n",
    "# Количество слоев\n",
    "K = 5\n",
    "# Количество нейронов на слой\n",
    "D = 6\n",
    "# Входной слой\n",
    "D_i = 1\n",
    "# Выходной слой\n",
    "D_o = 1\n",
    "\n",
    "# Сделаем пустые списки\n",
    "all_weights = [None] * (K+1)\n",
    "all_biases = [None] * (K+1)\n",
    "\n",
    "# Создадим входной и выходной слои\n",
    "all_weights[0] = np.random.normal(size=(D, D_i))\n",
    "all_weights[-1] = np.random.normal(size=(D_o, D))\n",
    "all_biases[0] = np.random.normal(size =(D,1))\n",
    "all_biases[-1]= np.random.normal(size =(D_o,1))\n",
    "\n",
    "# Создадим промежуточные слои\n",
    "for layer in range(1,K):\n",
    "  all_weights[layer] = np.random.normal(size=(D,D))\n",
    "  all_biases[layer] = np.random.normal(size=(D,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jZh-7bPXIDq4"
   },
   "outputs": [],
   "source": [
    "# Определим функцию активации ReLU (Rectified Linear Unit)\n",
    "def ReLU(preactivation):\n",
    "  activation = preactivation.clip(0.0)\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5irtyxnLJSGX"
   },
   "source": [
    "Теперь давайте запустим нашу случайную сеть.  Матрицы весов $\\boldsymbol\\Omega_{1\\ldots K}$ являются элементами списка \"all_weights\", а смещения $\\boldsymbol\\beta_{1\\ldots k}$ являются элементами списка \"all_biases\"\n",
    "\n",
    "Мы знаем, что нам понадобятся предварительные активации $\\mathbf{f}_{0\\ldots K}$ и активации $\\mathbf{h}_{1\\ldots K}$ для прямого прохода, поэтому мы также сохраним и вернем их.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LgquJUJvJPaN"
   },
   "outputs": [],
   "source": [
    "def compute_network_output(net_input, all_weights, all_biases):\n",
    "\n",
    "  # Получим количество слоев\n",
    "  K = len(all_weights) -1\n",
    "\n",
    "  # Сохраним предварительные активации на каждом слое в список \"all_f\",\n",
    "  # а активации - во второй список \"all_h\".\n",
    "  all_f = [None] * (K+1)\n",
    "  all_h = [None] * (K+1)\n",
    "\n",
    "\n",
    "  # Для удобства мы установим\n",
    "  # all_h[0] в качестве входа, а all_f[K] будет выходом\n",
    "  all_h[0] = net_input\n",
    "\n",
    "  # Пройдемся по слоям, вычисляя all_f[0...K-1] и all_h[1...K]\n",
    "  for layer in range(K):\n",
    "      # Обновите преактивации и активации на этом слое в соответствии с уравнением 7.16\n",
    "      # Не забудьте использовать np.matmul для умножения матриц\n",
    "      # TODO - Замените строки ниже\n",
    "      all_f[layer] = all_biases[layer] + np.matmul(all_weights[layer], all_h[layer])\n",
    "      all_h[layer+1] = ReLU(all_f[layer])\n",
    "\n",
    "\n",
    "\n",
    "  # Вычислите выход последнего скрытого слоя\n",
    "  # TO DO -- Замените строку ниже\n",
    "  all_f[K] = all_biases[K] + np.matmul(all_weights[K], all_h[K])\n",
    "\n",
    "  # Извлечем выход\n",
    "  net_output = all_f[K]\n",
    "\n",
    "  return net_output, all_f, all_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IN6w5m2ZOhnB",
    "outputId": "d559fc25-5c24-4e78-d317-75f14be1001c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True output = 1.907, Your answer = 1.907\n"
     ]
    }
   ],
   "source": [
    "# Зададим входные данные\n",
    "net_input = np.ones((D_i,1)) * 1.2\n",
    "# Рассчитаем выход нейронной сети\n",
    "net_output, all_f, all_h = compute_network_output(net_input,all_weights, all_biases)\n",
    "print(\"True output = %3.3f, Your answer = %3.3f\"%(1.907, net_output[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxVTKp3IcoBF"
   },
   "source": [
    "Теперь давайте зададим функцию потерь. Мы будем использовать квадратичную функцию потерь. Мы также напишем функцию для вычисления d_loss_d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6XqWSYWJdhQR"
   },
   "outputs": [],
   "source": [
    "def least_squares_loss(net_output, y):\n",
    "  return np.sum((net_output-y) * (net_output-y))\n",
    "\n",
    "def d_loss_d_output(net_output, y):\n",
    "    return 2*(net_output-y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njF2DUQmfttR",
    "outputId": "a96a2a34-9d8f-4054-e7d2-7dd67b04a859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 20.000 Loss = 327.371\n"
     ]
    }
   ],
   "source": [
    "y = np.ones((D_o,1)) * 20.0\n",
    "loss = least_squares_loss(net_output, y)\n",
    "print(\"y = %3.3f Loss = %3.3f\"%(y, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98WmyqFYWA-0"
   },
   "source": [
    "Теперь давайте вычислим производные сети. Мы уже вычислили прямой проход. Давайте вычислим обратный проход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LJng7WpRPLMz"
   },
   "outputs": [],
   "source": [
    "# Нам понадобится индикаторная функция\n",
    "def indicator_function(x):\n",
    "  x_in = np.array(x)\n",
    "  x_in[x_in>=0] = 1\n",
    "  x_in[x_in<0] = 0\n",
    "  return x_in\n",
    "\n",
    "# Основной расчет обратного прохода\n",
    "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
    "  # Мы также сохраним производные dl_dweights и dl_dbiases в списках\n",
    "  all_dl_dweights = [None] * (K+1)\n",
    "  all_dl_dbiases = [None] * (K+1)\n",
    "  # И сохраним производные от функции потерь относительно активации и предварительной активации в списках\n",
    "  all_dl_df = [None] * (K+1)\n",
    "  all_dl_dh = [None] * (K+1)\n",
    "\n",
    "  # Опять же, для удобства будем придерживаться соглашения о том, что all_h[0] - это вход сети, а all_f[k] - выход сети\n",
    "\n",
    "\n",
    "  # Вычислим производные от функции потерь по отношению к выходу сети\n",
    "  all_dl_df[K] = np.array(d_loss_d_output(all_f[K],y))\n",
    "\n",
    "\n",
    "  # Теперь пойдем в обратном направлении по сети\n",
    "  for layer in range(K,-1,-1):\n",
    "    # TODO Вычислите производные функции потерь относительно смещений на этом слое из all_dl_df[layer]. (уравнение 7.21)\n",
    "    # ПРИМЕЧАНИЕ.  Чтобы получить копию матрицы X, используйте Z=np.array(X)\n",
    "    # ЗАМЕНИТЕ ЭТУ СТРОКУ\n",
    "    all_dl_dbiases[layer] = np.array(all_dl_df[layer])\n",
    "\n",
    "\n",
    "\n",
    "    # TODO вычислите производные функции потерь относительно весов в слое из all_dl_df[layer] и all_h[layer] (уравнение 7.22)\n",
    "    # Не забудьте использовать np.matmul\n",
    "    # ЗАМЕНИТЕ ЭТУ СТРОКУ\n",
    "    all_dl_dweights[layer] = np.matmul(np.array(all_dl_df[layer]), all_h[layer].T)\n",
    "\n",
    "\n",
    "    # TODO: вычислите производные фунции потерь относительно активаций из весов и производных от следующих предварительных активаций (вторая часть последней строки уравнения 7.24)\n",
    "    # Не забудьте использовать np.matmul\n",
    "    # ЗАМЕНИТЕ ЭТУ СТРОКУ\n",
    "    all_dl_dh[layer] = np.matmul(all_weights[layer].T, all_dl_df[layer])\n",
    "\n",
    "    if layer > 0:\n",
    "      # TODO Вычислите производные функции потерь относительно предварительной активации f (используйте производную функции ReLU, первую часть последней строки уравнения 7.24)\n",
    "      # ЗАМЕНИТЕ ЭТУ СТРОКУ\n",
    "      all_dl_df[layer-1] = indicator_function(all_f[layer-1])*all_dl_dh[layer]\n",
    "\n",
    "  return all_dl_dweights, all_dl_dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9A9MHc4sQvbp"
   },
   "outputs": [],
   "source": [
    "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PK-UtE3hreAK",
    "outputId": "ce33a37a-f475-4e2c-8961-5796b7d7bc15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Bias 0, derivatives from backprop:\n",
      "[[ -4.486]\n",
      " [  4.947]\n",
      " [  6.812]\n",
      " [ -3.883]\n",
      " [-24.935]\n",
      " [  0.   ]]\n",
      "Bias 0, derivatives from finite differences\n",
      "[[ -4.486]\n",
      " [  4.947]\n",
      " [  6.812]\n",
      " [ -3.883]\n",
      " [-24.935]\n",
      " [  0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Bias 1, derivatives from backprop:\n",
      "[[ -0.   ]\n",
      " [-11.297]\n",
      " [  0.   ]\n",
      " [  0.   ]\n",
      " [-10.722]\n",
      " [  0.   ]]\n",
      "Bias 1, derivatives from finite differences\n",
      "[[  0.   ]\n",
      " [-11.297]\n",
      " [  0.   ]\n",
      " [  0.   ]\n",
      " [-10.722]\n",
      " [  0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Bias 2, derivatives from backprop:\n",
      "[[-0.   ]\n",
      " [-0.   ]\n",
      " [ 0.938]\n",
      " [ 0.   ]\n",
      " [-9.993]\n",
      " [ 0.508]]\n",
      "Bias 2, derivatives from finite differences\n",
      "[[ 0.   ]\n",
      " [ 0.   ]\n",
      " [ 0.938]\n",
      " [ 0.   ]\n",
      " [-9.993]\n",
      " [ 0.508]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Bias 3, derivatives from backprop:\n",
      "[[-0.   ]\n",
      " [-4.8  ]\n",
      " [-1.661]\n",
      " [-0.   ]\n",
      " [ 3.393]\n",
      " [ 5.391]]\n",
      "Bias 3, derivatives from finite differences\n",
      "[[ 0.   ]\n",
      " [-4.8  ]\n",
      " [-1.661]\n",
      " [ 0.   ]\n",
      " [ 3.393]\n",
      " [ 5.391]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Bias 4, derivatives from backprop:\n",
      "[[-0.   ]\n",
      " [ 0.   ]\n",
      " [ 0.   ]\n",
      " [-0.   ]\n",
      " [-5.212]\n",
      " [-0.   ]]\n",
      "Bias 4, derivatives from finite differences\n",
      "[[ 0.   ]\n",
      " [ 0.   ]\n",
      " [ 0.   ]\n",
      " [ 0.   ]\n",
      " [-5.212]\n",
      " [ 0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 0, derivatives from backprop:\n",
      "[[ -5.383]\n",
      " [  5.937]\n",
      " [  8.174]\n",
      " [ -4.66 ]\n",
      " [-29.922]\n",
      " [  0.   ]]\n",
      "Weight 0, derivatives from finite differences\n",
      "[[ -5.383]\n",
      " [  5.937]\n",
      " [  8.174]\n",
      " [ -4.66 ]\n",
      " [-29.922]\n",
      " [  0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 1, derivatives from backprop:\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]]\n",
      "Weight 1, derivatives from finite differences\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 2, derivatives from backprop:\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      5.371   0.      0.      3.145   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.    -57.233   0.      0.    -33.506   0.   ]\n",
      " [  0.      2.907   0.      0.      1.702   0.   ]]\n",
      "Weight 2, derivatives from finite differences\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      5.371   0.      0.      3.145   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.    -57.233   0.      0.    -33.506   0.   ]\n",
      " [  0.      2.907   0.      0.      1.702   0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 3, derivatives from backprop:\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.     -3.674   0.    -42.905 -10.998]\n",
      " [  0.      0.     -1.272   0.    -14.85   -3.807]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      2.597   0.     30.333   7.776]\n",
      " [  0.      0.      4.126   0.     48.188  12.353]]\n",
      "Weight 3, derivatives from finite differences\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.     -3.674   0.    -42.905 -10.998]\n",
      " [  0.      0.     -1.272   0.    -14.85   -3.807]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      2.597   0.     30.333   7.776]\n",
      " [  0.      0.      4.126   0.     48.188  12.353]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 4, derivatives from backprop:\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n",
      " [  0.      0.      0.      0.      0.      0.   ]]\n",
      "Weight 4, derivatives from finite differences\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n",
      " [  0.      0.      0.      0.      0.      0.   ]]\n",
      "Success!  Derivatives match.\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "# Создайте пустые списки для производных, вычисленных с помощью конечных разностей\n",
    "all_dl_dweights_fd = [None] * (K+1)\n",
    "all_dl_dbiases_fd = [None] * (K+1)\n",
    "\n",
    "# Давайте проверим, правильно ли мы вычисляем производные,\n",
    "# сравнив их со значениями, рассчитанными с помощью конечных разностей\n",
    "delta_fd = 0.000001\n",
    "\n",
    "# Проверим производные по векторам смещения\n",
    "for layer in range(K):\n",
    "  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n",
    "  # Для каждого элемента в смещении\n",
    "  for row in range(all_biases[layer].shape[0]):\n",
    "    # Сделаем копию смещений, мы будем менять по одному элементу каждый раз.\n",
    "    all_biases_copy = [np.array(x) for x in all_biases]\n",
    "    all_biases_copy[layer][row] += delta_fd\n",
    "    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n",
    "    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
    "    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
    "  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
    "  print(\"-----------------------------------------------\")\n",
    "  print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
    "  print(all_dl_dbiases[layer])\n",
    "  print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
    "  print(all_dl_dbiases_fd[layer])\n",
    "  if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
    "    print(\"Success!  Derivatives match.\")\n",
    "  else:\n",
    "    print(\"Failure!  Derivatives different.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Проверим производные по весовым матрицам\n",
    "for layer in range(K):\n",
    "  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n",
    "  # Для каждого элемента в all_weights\n",
    "  for row in range(all_weights[layer].shape[0]):\n",
    "    for col in range(all_weights[layer].shape[1]):\n",
    "      # Сделаем копию весов, мы будем менять по одному элементу каждый раз.\n",
    "      all_weights_copy = [np.array(x) for x in all_weights]\n",
    "      all_weights_copy[layer][row][col] += delta_fd\n",
    "      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n",
    "      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
    "      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
    "  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
    "  print(\"-----------------------------------------------\")\n",
    "  print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
    "  print(all_dl_dweights[layer])\n",
    "  print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
    "  print(all_dl_dweights_fd[layer])\n",
    "  if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
    "    print(\"Success!  Derivatives match.\")\n",
    "  else:\n",
    "    print(\"Failure!  Derivatives different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
